# 人工智能算法发展历史
<!--

                                    发展脉络总览

符号主义（1950s-1980s） --》 统计学习（1990s-2000s） --》 连接主义/深度学习（2010s-至今）
          |                         |                          |
          ∨                         ∨                          ∨
逻辑推理、专家系统           SVM、决策树、贝叶斯         CNN、RNN、Transformer

-->

## 二、分阶段算法演进与关系

### 1. 符号主义（Symbolic AI）
- **时期**：1950s-1980s  
- **核心思想**：智能=符号操作 + 逻辑规则  
- **代表算法**：  
  - **逻辑推理系统（1950s）**：基于一阶谓词逻辑（如纽厄尔的逻辑理论家）  
  - **专家系统（1970s）**：MYCIN（医疗诊断）、DENDRAL（化学分析）  
- **优点**：可解释性强，适合规则明确的领域  
- **局限**：依赖人工设计规则，无法处理不确定性  
- **衰落原因**：规则维护成本高，难以泛化  

### 2. 连接主义（Connectionism）的早期探索
- **时期**：1950s-1980s（与符号主义并行）  
- **核心思想**：智能=分布式神经网络 + 学习  
- **代表算法**：  
  - **感知机（1957）**：单层线性分类器，无法处理非线性问题 → 被明斯基批判后陷入低谷  
  - **Hopfield网络（1982）**：引入能量函数，用于联想记忆  
  - **反向传播算法（1986）**：解决多层神经网络训练问题，但受限于算力和数据  
- **优点**：具备学习能力，适合模式识别  
- **局限**：训练不稳定（梯度消失），算力需求高  

### 3. 统计学习（Statistical Learning）
- **时期**：1990s-2000s  
- **核心思想**：智能=数据驱动 + 概率模型  
- **代表算法**：  
  - **支持向量机（SVM）（1995）**：通过核函数处理非线性分类，取代早期神经网络  
  - **决策树与随机森林（1984/2001）**：可解释性强，适合结构化数据  
  - **贝叶斯网络（1980s）**：处理不确定性推理（如垃圾邮件过滤）  
- **优点**：理论成熟，适合中小规模数据  
- **局限**：特征工程依赖人工，难以处理高维非结构化数据（如图像）  

### 4. 深度学习革命（Deep Learning）
- **时期**：2010s-至今  
- **核心思想**：智能=深度神经网络 + 端到端学习  
- **代表算法**：  
  - **卷积神经网络（CNN）（2012 AlexNet）**：局部感知、参数共享，颠覆图像识别  
  - **循环神经网络（RNN/LSTM）（1997/2014）**：处理序列数据（如文本、语音）  
  - **Transformer（2017）**：自注意力机制，取代RNN成为NLP主流（如BERT、GPT）  
  - **强化学习（RL）（2016 AlphaGo）**：通过试错学习策略（游戏、机器人控制）  
- **优点**：自动提取特征，处理高维数据（图像、语音、文本）  
- **局限**：数据与算力需求大，可解释性差  

### 5. 现代融合趋势
- **神经符号系统（Neuro-Symbolic AI）**：结合符号逻辑与神经网络（如DeepMind的PrediNet）  
- **生成模型（Generative Models）**：GAN（生成对抗网络）、扩散模型（如Stable Diffusion）  
- **大模型（Large Models）**：GPT-4、PaLM，通过海量参数实现通用任务处理  

---

## 三、关键算法对比表
| **算法类型**       | **代表模型**   | **优势**                | **劣势**                | **适用场景**           |
|--------------------|----------------|-------------------------|-------------------------|-----------------------|
| 符号主义           | 专家系统       | 可解释性强              | 规则维护成本高          | 医疗诊断、化学分析    |
| 统计学习           | SVM、随机森林  | 中小数据高效            | 依赖特征工程            | 结构化数据分类        |
| 深度学习（CNN）    | ResNet         | 自动提取图像特征        | 需要大量标注数据        | 图像识别、目标检测    |
| 深度学习（Transformer） | BERT、GPT      | 长距离依赖建模          | 计算资源消耗大          | 自然语言处理          |
| 强化学习           | AlphaGo        | 复杂策略优化            | 训练不稳定、耗时        | 游戏、机器人控制      |

---

## 四、算法演进的推动力
1. **数据**：从小规模标注数据（SVM）到互联网级大数据（深度学习）  
2. **算力**：从CPU到GPU/TPU的并行计算革命  
3. **理论突破**：反向传播、注意力机制、残差连接  
4. **应用需求**：从规则明确的专家任务（医疗）到开放环境感知（自动驾驶）  

---

## 五、总结：如何选择算法？
- **数据量小+可解释性要求高** → 统计学习（如随机森林）  
- **图像/语音/文本处理** → 深度学习（CNN/Transformer）  
- **动态决策与交互** → 强化学习  
- **规则明确且稳定** → 符号系统（现代通常与神经网络结合）  