# 无人机集群协同搜索研究综述——笔记

## 一、基于深度 Q 网络的协同搜索算法

### 算法概述

**深度 Q 网络（Deep Q Network, DQN）** 是一种结合了深度学习和强化学习的算法。通过将深度卷积神经网络与 Q 学习相结合，DQN 能够高效处理高维度、连续状态空间中的决策问题。

### 核心思想

1. **深度卷积神经网络（DCNN）**  
   利用深度卷积神经网络的强大特征提取能力，从环境的原始输入（如图像或其他高维数据）中提取关键信息。

2. **Q 学习与马尔科夫决策过程（MDP）**  
   - 使用 Q 学习方法计算状态-动作对的 Q 值，评估每个动作的长期回报。  
   - 基于马尔科夫决策过程，通过强化学习生成策略。

3. **状态到动作的映射**  
   根据 Q 值选择最优动作（或通过 \( \epsilon \)-greedy 策略进行探索），实现从环境状态到动作的映射。

4. **经验回放与目标网络**  
   - 使用经验回放机制，随机采样历史数据进行训练，打破数据间的相关性，提升训练稳定性。  
   - 引入目标网络，稳定目标 Q 值计算过程，防止网络参数快速震荡。

---

### 算法流程

#### 1. 初始化

- 初始化 Q 网络参数 \( \theta \)。  
- 初始化目标网络 \( \theta^- \)（定期与 \( \theta \) 同步）。  
- 创建经验回放池 \( D \)。  
- 设置超参数：折扣因子 \( \gamma \)、学习率、探索率 \( \epsilon \) 等。

---

#### 2. 感知环境并提取特征

- 智能体从环境中接收状态 \( s_t \)。  
- 使用卷积神经网络提取状态的低维特征。

---

#### 3. 动作选择

基于 \( \epsilon \)-greedy 策略选择动作 \( a_t \)：
\[
a_t = 
\begin{cases} 
\text{随机动作} & \text{以概率 } \epsilon \\
\arg\max_a Q(s_t, a; \theta) & \text{以概率 } 1-\epsilon
\end{cases}
\]

---

#### 4. 环境交互

- 智能体执行动作 \( a_t \)，环境反馈：  
  - 新状态 \( s_{t+1} \)，  
  - 奖励 \( r_t \)，  
  - 是否结束标志 \( done \)。

- 将交互数据 \( (s_t, a_t, r_t, s_{t+1}, done) \) 存储到经验池 \( D \)。

---

#### 5. 网络训练

1. **采样经验**  
   从经验池 \( D \) 中随机采样小批量数据 \( (s_i, a_i, r_i, s_{i+1}, done_i) \)。

2. **计算目标 Q 值**  
   \[
   y_i = 
   \begin{cases} 
   r_i & \text{若 } done_i = \text{True} \\
   r_i + \gamma \max_a Q(s_{i+1}, a; \theta^-) & \text{若 } done_i = \text{False}
   \end{cases}
   \]

3. **优化损失函数**  
   \[
   L(\theta) = \frac{1}{N} \sum_i \left( y_i - Q(s_i, a_i; \theta) \right)^2
   \]

4. **更新网络参数**  
   使用随机梯度下降或其他优化器更新 Q 网络参数 \( \theta \)。

---

#### 6. 目标网络更新

每隔固定步数，将 Q 网络的参数 \( \theta \) 同步到目标网络 \( \theta^- \)：  
\[
\theta^- \gets \theta
\]

---

#### 7. 迭代与终止

- 持续执行上述步骤，直到满足终止条件（如目标达到或最大训练步数）。  
- 输出最优策略。

---

### 伪代码描述

```python
# 初始化
Initialize Q-network with weights θ
Initialize target Q-network with weights θ⁻ ← θ
Initialize experience replay memory D
Set hyperparameters: γ, ε, learning rate, etc.

# 主循环
for episode in range(max_episodes):
    Initialize environment state s₀
    for t in range(max_steps):
        # 动作选择
        if random() < ε:
            aₜ ← Random Action
        else:
            aₜ ← argmax(Q(sₜ, a; θ))

        # 执行动作并获取反馈
        sₜ₊₁, r, done ← Environment.step(aₜ)

        # 存储经验
        D.add(sₜ, aₜ, r, sₜ₊₁, done)

        # 经验回放与训练
        Batch ← RandomSample(D)
        for (s, a, r, s_next, done) in Batch:
            if done:
                y ← r
            else:
                y ← r + γ * max(Q(s_next, a; θ⁻))
            Update θ to minimize (y - Q(s, a; θ))²

        # 更新目标网络
        if t % target_update_freq == 0:
            θ⁻ ← θ

        if done:
            break

    # 动态调整 ε
    ε ← max(ε_min, ε_decay * ε)

# 输出最终策略
return Q-network
```


## 二、基于 QMIX 网络的协同搜索算法

### 算法概述

**QMIX** 是一种多智能体强化学习（MARL）算法，利用联合 Q 值分解实现智能体间高效协作，适用于需要紧密协作的任务场景。

### 核心思想

1. **全局 Q 值分解**  
   全局 Q 值 \( Q_{tot} \) 是各智能体局部 Q 值的非线性可加函数：
   \[
   Q_{tot} = f(Q_1, Q_2, \ldots, Q_n; \mathbf{s})
   \]
   其中 \( Q_i \) 是第 \( i \) 个智能体的局部 Q 值，\( \mathbf{s} \) 是全局状态。

2. **可加性约束**  
   QMIX 保证 **个体最优性** 和 **联合最优性** 的一致性，即个体的最优策略组合能产生全局最优策略。

3. **混合网络（Mixing Network）**  
   混合网络通过非负约束生成全局 Q 值，从而实现智能体间的协同决策。

---

### 算法流程

#### 1. 初始化

- 定义智能体集合 \( \mathcal{A} = \{A_1, A_2, \ldots, A_n\} \)。  
- 初始化每个智能体的局部 Q 网络 \( Q_i(s, a_i; \theta_i) \)。  
- 初始化全局混合网络的参数 \( \phi \)。  
- 创建经验回放池 \( D \)。  
- 设置超参数：折扣因子 \( \gamma \)、学习率、探索率 \( \epsilon \) 等。

---

#### 2. 多智能体协作

1. **环境感知**  
   每个智能体从环境中获取局部观测值 \( o_i \)，并共享部分或全部信息以形成全局状态 \( s \)。

2. **动作选择（局部策略）**  
   基于 \( \epsilon \)-greedy 策略选择动作：
   \[
   a_i = 
   \begin{cases} 
   \text{随机动作} & \text{以概率 } \epsilon \\
   \arg\max_{a_i} Q_i(o_i, a_i; \theta_i) & \text{以概率 } 1-\epsilon
   \end{cases}
   \]

---

#### 3. 环境交互与存储

- 每个智能体执行动作 \( a_i \)，环境返回奖励 \( r \) 和新的局部状态 \( o_i' \)。  
- 将交互数据 \( (s, \mathbf{a}, r, s', done) \) 存储到经验回放池 \( D \)。  
  其中，\( \mathbf{a} = \{a_1, a_2, \ldots, a_n\} \)。

---

#### 4. QMIX 网络训练

1. **采样经验**  
   从经验池中随机采样小批量数据 \( (s, \mathbf{a}, r, s', done) \)。

2. **局部 Q 值计算**  
   每个智能体的局部 Q 网络预测局部 Q 值：
   \[
   Q_i(o_i, a_i; \theta_i)
   \]

3. **全局 Q 值生成**  
   混合网络根据局部 Q 值生成全局 Q 值：
   \[
   Q_{tot} = f(Q_1, Q_2, \ldots, Q_n; \phi)
   \]

4. **目标 Q 值计算**  
   \[
   y = 
   \begin{cases} 
   r & \text{若 } done = \text{True} \\
   r + \gamma Q_{tot}(s', \mathbf{a'}; \phi) & \text{若 } done = \text{False}
   \end{cases}
   \]
   其中，\( \mathbf{a'} = \arg\max_{\mathbf{a'}} Q_{tot}(s', \mathbf{a'}; \phi) \)。

5. **损失函数优化**  
   通过最小化均方误差更新参数：
   \[
   L(\theta, \phi) = \frac{1}{N} \sum_{i=1}^N \left( y_i - Q_{tot}(s_i, \mathbf{a_i}; \phi) \right)^2
   \]

6. **更新网络**  
   使用反向传播算法更新局部 Q 网络 \( \theta_i \) 和混合网络 \( \phi \)。

---

#### 5. 迭代与终止

- 重复步骤 2–4，直到满足终止条件（如搜索完成或达到最大步数）。  
- 输出训练好的局部 Q 网络和混合网络。

---

### 伪代码描述

```python
# 初始化
Initialize local Q-networks {Q_i(o_i, a_i; θ_i)} for all agents
Initialize mixing network with parameters φ
Initialize experience replay memory D
Set hyperparameters: γ, ε, learning rate, etc.

# 主循环
for episode in range(max_episodes):
    Initialize environment state s₀
    for t in range(max_steps):
        # 多智能体动作选择
        for agent i in agents:
            if random() < ε:
                a_i ← Random Action
            else:
                a_i ← argmax(Q_i(o_i, a_i; θ_i))

        # 执行动作并获取反馈
        sₜ₊₁, r, done ← Environment.step({a₁, a₂, ..., a_n})

        # 存储经验
        D.add(sₜ, {a₁, a₂, ..., a_n}, r, sₜ₊₁, done)

        # 经验回放与训练
        if len(D) > batch_size:
            Batch ← RandomSample(D)
            for (s, a, r, s_next, done) in Batch:
                # 计算局部 Q 值和目标 Q 值
                Q_local ← {Q_i(o_i, a_i; θ_i) for i in agents}
                Q_tot ← MixingNetwork(Q_local; φ)
                if done:
                    y ← r
                else:
                    Q_next_local ← {max(Q_i(o_i', a_i'; θ_i)) for i in agents}
                    Q_next_tot ← MixingNetwork(Q_next_local; φ)
                    y ← r + γ * Q_next_tot
                
                # 更新损失函数
                L ← MeanSquaredError(y, Q_tot)
                Update θ_i and φ to minimize L

        if done:
            break

    # 动态调整 ε
    ε ← max(ε_min, ε_decay * ε)

# 输出最终策略
return Q-networks and mixing network
```


## 三、基于分布式强化学习算法的集群协同搜索

### 算法概述

分布式强化学习通过将强化学习过程分解为多个智能体的并行执行，实现高效学习和决策。集群协同搜索结合了分布式计算和多智能体系统的协作能力，适用于处理动态、多目标和大规模任务场景。

### 核心思想

1. **分布式学习框架**  
   - 将强化学习任务分布到多个智能体，每个智能体独立与环境交互并更新局部策略。  
   - 通过参数同步或经验共享实现智能体之间的协作。

2. **并行训练**  
   - 多个智能体并行运行，提高计算效率并加速策略收敛。  
   - 使用分布式优化算法（如 A3C、IMPALA 等）协调学习过程。

3. **集群协同**  
   - 智能体之间共享局部环境信息，形成全局视图。  
   - 通过策略融合和决策协作，优化集群搜索效果。

---

### 算法流程

#### 1. 初始化

- 定义智能体集合 \( \mathcal{A} = \{A_1, A_2, \ldots, A_n\} \)。  
- 初始化全局共享策略 \( \pi_{\text{global}} \) 和每个智能体的局部策略 \( \pi_i \)。  
- 设置分布式架构，如参数服务器、工作节点、通信模块等。  
- 定义奖励函数 \( R(s, a) \)、折扣因子 \( \gamma \) 和其他超参数。

---

#### 2. 环境交互与并行探索

1. **局部探索**  
   - 每个智能体从环境中感知状态 \( s_i \)，基于其局部策略 \( \pi_i \) 执行动作 \( a_i \)。  
   - 获取环境反馈 \( r_i \) 和新状态 \( s_i' \)。  

2. **经验存储**  
   - 智能体将交互记录 \( (s_i, a_i, r_i, s_i') \) 存储到本地经验池或发送至全局经验池。

---

#### 3. 局部学习与参数同步

1. **局部策略更新**  
   - 每个智能体独立采样经验，通过梯度下降更新其局部策略 \( \pi_i \)。  
   - 基于不同算法，更新方式可能包括 Q 学习、策略梯度等。

2. **全局参数同步**  
   - 定期将局部策略的参数发送至全局服务器。  
   - 全局服务器聚合参数（如加权平均）并广播回智能体，更新局部策略：
     \[
     \pi_i \gets \pi_{\text{global}}
     \]

---

#### 4. 集群协同与决策优化

1. **信息共享**  
   - 智能体之间共享局部环境信息 \( \{o_1, o_2, \ldots, o_n\} \)，形成全局状态 \( s \)。  
   - 使用共享信息优化每个智能体的动作选择。

2. **协同决策**  
   - 利用共享的策略 \( \pi_{\text{global}} \) 进行联合行动选择：  
     \[
     \mathbf{a} = \{\arg\max_{a} \pi_{\text{global}}(s, a)\}
     \]

3. **冲突避免**  
   - 引入优先级调度或惩罚机制，避免智能体间的冲突（如资源竞争或路径重叠）。

---

#### 5. 终止条件

- 持续执行上述步骤，直到满足以下条件之一：  
  - 达到目标区域或覆盖目标点。  
  - 达到最大训练轮次或步数。

---

### 伪代码描述

```python
# 初始化
Initialize global policy π_global
Initialize local policies {π_i} for all agents
Set hyperparameters: γ, learning rate, sync interval, etc.
Define distributed architecture (e.g., parameter server, workers)

# 主循环
for episode in range(max_episodes):
    Parallel for each agent A_i:
        Initialize environment state s₀
        for t in range(max_steps):
            # 局部策略选择动作
            a_i ← π_i(s_i)

            # 与环境交互
            s_i', r_i, done ← Environment.step(a_i)

            # 存储经验
            Store (s_i, a_i, r_i, s_i') in local memory

            # 局部策略更新
            if len(local memory) > batch_size:
                Sample batch from local memory
                Update π_i using policy gradient or Q-learning

            if done:
                break

    # 参数同步
    if episode % sync_interval == 0:
        Aggregate local policies t
```


## QMIX 与分布式强化学习的区别

### QMIX 与分布式强化学习在局部与全局信息处理方面的区别

---

### **1. 局部信息与全局信息的处理**

#### **1.1 QMIX**

##### **局部信息处理**
- 每个智能体有一个局部观测 \( o_i \)，仅包含智能体局部环境的感知数据（如位置、邻近障碍物、目标等）。
- 智能体通过局部 Q 网络 \( Q_i(o_i, a_i; \theta_i) \) 计算每个动作 \( a_i \) 的价值。
- 局部信息通过 Q 值映射处理，但不直接共享给其他智能体。

##### **全局信息处理**
- 在训练阶段，QMIX 聚合所有智能体的局部 Q 值和全局状态 \( s \)，通过混合网络生成全局 Q 值 \( Q_{tot} \)：  
  \[
  Q_{tot} = f(Q_1, Q_2, \ldots, Q_n; \mathbf{s})
  \]
- 混合网络只在训练时使用，执行阶段各智能体独立决策。

---

#### **1.2 分布式强化学习**

##### **局部信息处理**
- 每个智能体从环境中感知局部状态 \( o_i \)，通过局部策略网络 \( \pi_i(a_i | o_i; \theta_i) \) 进行决策。
- **信息共享机制**：  
  - **无共享模式**：智能体独立决策，适用于异质任务。  
  - **共享模式**：通过通信模块共享局部状态，用于形成全局视图。

##### **全局信息处理**
- 全局信息通过显式通信模块或参数服务器汇总，例如共享全局状态或奖励信号。
- 分布式强化学习依赖参数聚合机制更新全局策略：  
  \[
  \theta_{\text{global}} = \frac{1}{N} \sum_{i=1}^N \theta_i
  \]
- 全局信息处理对架构的通信效率要求较高。

---

### **2. 决策机制**

#### **2.1 QMIX**

- **局部决策**  
  智能体独立计算 \( Q_i(o_i, a_i; \theta_i) \)，通过 \( \epsilon \)-greedy 策略选择动作：  
  \[
  a_i = 
  \begin{cases} 
  \text{随机动作} & \text{以概率 } \epsilon \\
  \arg\max_{a_i} Q_i(o_i, a_i; \theta_i) & \text{以概率 } 1-\epsilon
  \end{cases}
  \]
- **全局最优性保障**  
  混合网络引入单调性约束，确保 \( Q_{tot} \) 是所有局部 Q 值的非减函数：
  \[
  \frac{\partial Q_{tot}}{\partial Q_i} \geq 0
  \]
  这保证个体最优策略组合能够提升全局 Q 值，间接实现全局最优。

---

#### **2.2 分布式强化学习**

- **局部决策**  
  每个智能体独立基于局部策略 \( \pi_i(a_i | o_i) \) 决策：  
  \[
  a_i \sim \pi_i(a_i | o_i; \theta_i)
  \]
  如果启用信息共享，决策可参考其他智能体的状态或动作。

- **全局最优性保障**  
  通过以下方式协调智能体决策：  
  - **共享奖励**：共享奖励信号，鼓励协作。  
  - **全局策略聚合**：通过参数服务器或通信模块同步更新全局策略：  
    \[
    \theta_{\text{global}} = \text{Aggregate}(\{\theta_i\})
    \]
  - **异步优化**：允许智能体在动态场景中异步更新策略。

---

### **3. 达到最优解的方式**

#### **3.1 QMIX**

##### **方式**
1. **中心化优化**：  
   训练阶段利用全局状态和混合网络，确保局部 Q 值组合生成全局最优策略。
2. **单调性约束**：  
   确保 \( Q_{tot} \) 单调依赖局部 Q 值，避免个体最优对全局优化的干扰。
3. **执行阶段**：  
   各智能体基于局部 Q 网络独立决策，策略优劣取决于训练效果。

##### **适用场景**
- **小规模场景**：如机器人编队、多目标跟踪。

---

#### **3.2 分布式强化学习**

##### **方式**
1. **并行探索与训练**：  
   智能体并行与环境交互，通过共享经验或分布式梯度更新策略。
2. **全局策略聚合**：  
   参数服务器定期同步各智能体的策略，形成全局最优解。
3. **通信与协作**：  
   信息共享机制（如状态和奖励）优化智能体间的协作。

##### **适用场景**
- **大规模场景**：如无人机集群搜索、智能交通系统。

---

### **4. 总结对比**

| **属性**               | **QMIX**                                           | **分布式强化学习**                              |
|------------------------|---------------------------------------------------|-----------------------------------------------|
| **信息共享**            | 隐式共享（通过混合网络聚合局部 Q 值）               | 显式共享（局部状态或参数同步）                |
| **全局视图**            | 中心化训练引入全局视图                              | 依赖通信模块或参数服务器构建全局视图           |
| **全局最优保障**         | 基于混合网络的单调性约束                             | 依靠全局策略聚合和共享奖励                   |
| **适用场景**            | 小规模多智能体协作场景                              | 大规模分布式任务，环境动态复杂                |

---

### **5. 技术优劣势对比**

| **技术点**             | **QMIX 的优势**                                   | **分布式强化学习的优势**                       |
|-----------------------|-------------------------------------------------|-----------------------------------------------|
| **训练效率**           | 利用中心化训练更快收敛                           | 并行训练显著提升效率                         |
| **策略质量**           | 保证局部和全局最优策略的一致性                   | 策略质量依赖通信效率和同步机制               |
| **信息共享方式**       | 通过混合网络隐式共享信息                         | 信息显式共享，智能体间通信依赖强              |
| **部署灵活性**         | 执行时无需中心化计算                             | 支持完全分布式部署，适应复杂环境              |

---

### **6. 总结**

1. **QMIX**  
   - 偏重中心化优化，通过混合网络隐式协作实现全局最优。  
   - 适合小规模高协作场景，如机器人编队或团队任务。

2. **分布式强化学习**  
   - 注重并行性和灵活性，通过显式信息共享和参数同步实现协作。  
   - 更适合处理大规模动态环境，如无人机集群搜索和智能交通。

选择算法需根据任务规模、环境动态性和智能体协作需求具体分析。
